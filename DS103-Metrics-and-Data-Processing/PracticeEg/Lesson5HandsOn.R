# HandsOn on Factor Analysis and Reliability testing

# Load Libraries
library("corpcor")
library("GPArotation")
library("psych")
library("IDPmisc")
library("plyr")
library("readr")


# Import Data
studentSurvey <- read_csv("Documents/GitHub/DS-Student-Resources/DS103-Metrics-and-Data-Processing/PracticeEg/studentSurvey.csv")
View(studentSurvey)

# Question Setup

# 1. Whether this data is suitable for factor analysis.

# With the data above, we will be determining how a set of questions from the student survey hang together 
# and whether there are any subscales.

# Data Wrangling

# We'll need to trim our data to only the variables we are interested in looking at to begin with.
# In order to subset, we want the items that labeled Area1 through Area12.

# They are contained in columns numbered 31-42.

studentSurveySub1 <- studentSurvey[, 31:42]

# Omit the empty rows

studentSurveySub1 <- na.omit(studentSurveySub1)

# Test Assumptions

# We'll will be looking at sample size and how well the variables relate to each other.

# Sample Size

nrow(studentSurveySub1)

# Sample size should ideally be 300 or more. we have 464 here, so we have met the SampleSize assumption!

# Absence of Multicollinearity

# Next, we will test for the absence of multicollinearity. The first way to do this is with a correlation matrix. 
# We can use the function cor() to do that:

studentSurveyMatrix <- cor(studentSurveySub1)
View(round(studentSurveyMatrix, 2)) # The 2 in the code indicates the number of decimal places 

# In it, we want to look at only half the matrix. As we go down the columns, starting to look only after the 1.0
# on the diagonal, look for any correlations that are higher than .9.This would indicate really high multicollinearity,
# and if there's an item that has a correlation of .9, we will most likely want to remove that item. 

# A quick scan indicates that there is nothing above .9 here and we are good to go.

# Some Relationship between Survey Items

# We will also want to look at the correlation matrix to ensure that correlations aren't too low, 
# since factor analysis requires some relationship between the variables. Look for any variable that
# correlates with more than one variable lower than .3. 

#Again, this doesn't seem to be a problem in the matrix above, so we are good to go!
  
# Bartlett's Test

# To double check our findings from the correlation matrix, we can also run Bartlett's test:

cortest.bartlett(studentSurveySub1)


# Check your Determinants

# It's another measure of how variables relate to each other.

det(studentSurveyMatrix)

# If this value(0.002180645 in this case) is greater than .00001 , then again, we have a sufficient relation between our variables
# to proceed with a factor analysis. With all ways to test - correlation matrix, Bartlett's test,
# and determinants - we are good to go! 

# 2. The appropriate number of factors for this data

# Initial Pass to Determine Approximate Number of Factors

# The first thing we will do is to run a basic factor analysis
# with as many factors as we have survey items in our factor analysis, and without any rotation. 

pcModel1 <- principal(studentSurveySub1, nfactors = 12, rotate = "none") # We have 12 factors
pcModel1

# What you we really looking at here on the first pass is the SS loadings column on the bottom, 
# which contains something called eigenvalues. The larger the eigenvalue, the more likely that the factor is important. 
# Typically there is a cutoff of 1, so if we see a factor with an eigenvalue of > 1,
# chances are it's something real to examine.

# Here we have one factor 6.48, which is > 1

# Examine the Scree Plot

# We are plotting the eigenvalues generated by our model, and the argument type="b" 
# shows both a line and points on the same graph.

plot(pcModel1$values, type="b")

# In this case, we can see that there is quite a jump down between 1 and 2. The rest really seem to trail off 
# after that. This info, along with examining
# the eigenvalues, tells us that probably there is one factor. So we can now test that assumption,
# to see if the model fit improves with one factor.

# Second Pass to Test the Suspected Number of Factors
# Now, we'll run similar code again, but this time, we will change nfactors= to 1 instead of 12:

pcModel2 <- principal(studentSurveySub1, nfactors = 1, rotate = "none")
pcModel2

# 3. The best model fit for this data
# Examining Residuals to Determine Model Fit

# The basic idea behind this test is that the model
# fits our data very well if there is very little difference between the correlation matrix and the 
# loadings generated through our model. The difference between them is known as the residual.

residuals <- factor.residuals(studentSurveyMatrix, pcModel2$loadings)

residuals <- as.matrix(residuals[upper.tri(residuals)])

largeResid <- abs(residuals) > .05

sum(largeResid)
sum(largeResid/nrow(residuals)) # The output is .4545

# Meaning that 45% of residuals are large which is still under 50%. So having only one factors is a good model fit for the data.

# 4. Rotate the Factors to Determine Where Each Survey Item Fits

# Oblique Rotation

pcModel3 <- principal(studentSurveySub1, nfactors = 1, rotate = "oblimin")
pcModel3

# We now have access to what is called a pattern matrix, and the print outs in the column
# PC1 tell us how well each item fits into that factor. The values can range from 0 to 1,
# and the larger the value, the better the fit. 

print.psych(pcModel3, cut = .3, sort=TRUE)

# From this, we can see that we get the exact same results.

# Orthogonal Rotation

pcModel4 <- principal(studentSurveySub1, nfactors = 1, rotate = "varimax")
print.psych(pcModel4, cut=.3, sort=TRUE)

# We can see that results are very similar in terms of where items fit on the factors.


#######################

# CALCULATING RELAIABILITY

library("psych")

# Question Setup
#   
# Is my survey reliable? Does it measure the same thing every time?

# Data Wrangling

# Subsetting the data. 
# In this case we already have studentSurveySub1, from above.
# studentSurveySub1 <- studentSurvey[, 31:42]

# Test Assumptions

# Calculating Reliability
# Just make use of alpha() and place as the sole argument the data frame that contains our data! 

alpha(studentSurveySub1)

# Interpret Output for Inter-Rater Reliability
# 5. Whether the scale is considered reliable through inter-rater reliability

# We notice that the raw_alpha score for studentSurveySub1 is alpha = .92, so the scale has pretty good inter-rater reliability.

# Interpret Output for Inter-Item Reliability
# 6. Whether the scale is considered reliable through inter-item reliability

# This information can be found both in the Item statistics and the Non missing response frequency for 
# each item sections. Under Item statistics we looking for all of these r.drop values to be above .3. 

# In the Non missing response frequency table, we will find the percentage of people who gave each response 
# option for each item. The idea here is just to make sure that everyone is not answering all one way for any 
# particular item. It should be relatively evenly distributed, or at the very least, not clumped up! There's a 
# pretty good chance that something is wrong with your question if absolutely everyone is answering the question 
# the same way!

# So the scale is considered reliable through inter-rater reliability as well.















